{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology and notation\n",
    "\n",
    "#### Inputs\n",
    "- $ s_t $ - state \n",
    "- $ o_t $ - observation\n",
    "- $ a_t $ - action\n",
    "\n",
    "#### Outputs\n",
    "- $ \\pi_\\theta(a_t|o_t) $ - policy\n",
    "- $ \\pi_\\theta(a_t|s_t) $ - policy (fully observed)\n",
    "\n",
    "### Mappings\n",
    "- $ r(s, a) $: reward function\n",
    "\n",
    "### Markov Decision Processes\n",
    "\n",
    "#### Markov chain\n",
    "- $ M = \\{S, T\\} $\n",
    "- $ S - $ state space, states $ s \\in S $ (discrete or continuous)\n",
    "- $ T - $ transition operator, transition probability, dynamics function, $ p(s_{t+1}|s_t) $\n",
    "    - let $ \\mu_{t, i} = p(s_t = i) $ $ \\vec{\\mu_t} $ is a vector of probabilities\n",
    "    - let $ T_{i, j} = p(s_{t+1}=i \\lvert s_t = j) $, then $ \\vec{\\mu_{t+1}} = T\\vec{\\mu_t}$\n",
    "    - T is a linear operator applied on the current vector of state probabilities to produce the next vector of state\\                 probabilities. (2d matrix)\n",
    "\n",
    "#### Markov property\n",
    "- Given $ s_t $, $ s_{t+1} $ is independent of $ s_{t-1} $\n",
    "\n",
    "- Markov chain by itself does not allow us to specify a decision making problem because there is no notion of actions.\\\n",
    "    (pioneered in 1950s.)\n",
    "    ![image.png](..\\images\\rl_basics\\markov_chain.png)\n",
    "    \n",
    "#### Markov Decision Process\n",
    "- $ M = \\{S, A, T, r\\} $\n",
    "- $ S - $ state space, states $ s \\in S $ (discrete or continuous)\n",
    "- $ A - $ action space, actions $ a \\in A $ (discrete or continuous)\n",
    "- $ T - $ transition operator, transition probability, dynamics function, $ p(s_{t+1}|s_t, a_t) $ (now a tensor, 3d matrix)\n",
    "- $ r - $ reward function, $ r: S x A \\rightarrow \\mathbb{R} $\n",
    "    ![image.png](..\\images\\rl_basics\\markov_decision_process.png)\n",
    "\n",
    "\n",
    "#### Parially observed Markov Decision Process\n",
    "- $ M = \\{S, \\mathcal{A}, \\mathcal{O}, T, \\mathcal{E}, r\\} $\n",
    "- $ S - $ state space, states $ s \\in S $ (discrete or continuous)\n",
    "- $ \\mathcal{A} - $ action space, actions $ a \\in \\mathcal{A} $ (discrete or continuous)\n",
    "- $ \\mathcal{O} - $ observation space, observations $ o \\in \\mathcal{O} $ (discrete or continuous)\n",
    "- $ T - $ transition operator, transition probability, dynamics function, $ p(s_{t+1}|s_t, a_t) $ (now a tensor, 3d matrix)\n",
    "- $ \\mathcal{E} - $ emission probability $ p(o_t|s_t)$\n",
    "- $ r - $ reward function, $ r: S x \\mathcal{A} \\rightarrow \\mathbb{R} $\n",
    "    ![image.png](..\\images\\rl_basics\\partially_observed_markov_decision_process.png)\n",
    "- We will be making decisions based on observations without access to the true states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of reinforcement learning\n",
    "\n",
    "#### To learn a policy\n",
    "- $ \\pi_{\\theta}(a|s): s \\rightarrow a $\n",
    "\n",
    "#### Simulate environment dynamics to produce the next state\n",
    "- $ p(s^{'}| s, a): s, a \\rightarrow s^{'} $\n",
    "\n",
    "#### Joint  probability distribution over states and actions (trajectory distribution) for a finite time horizon\n",
    "- $ p_\\theta(s_1, a_1, \\dots, s_T, a_T )$ = $ p(s_1) \\prod\\limits_{t=1}^T \\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$\n",
    "\n",
    "#### Optimal policy\n",
    "- $ \\theta^{\\star} = arg \\max\\limits_{\\theta} E_{T \\sim p_{\\theta}(\\tau) }[\\sum\\limits_t r(s_t, a_t)] $\n",
    "- $ \\tau $ stands for the trajectory. trajectories are sequences of states and actions.\n",
    "- $ \\theta $ represents the parameter of a neural network.\n",
    "- expectation accounts for the stochasticity of the policy, transition probabilities, and the initial state distribution.\n",
    "- expected value under the trajectory distribution of the sum of rewards.\n",
    "\n",
    "#### Finite horizon case: state-action marginal using augmented  markov state\n",
    "\\begin{align}\n",
    "\\theta^{\\star} = arg \\max\\limits_{\\theta} E_{T \\sim p_{\\theta}(\\tau) }\\left[ \\sum\\limits_t r(s_t, a_t) \\right]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\theta^{\\star} = arg \\max\\limits_{\\theta} \\sum\\limits_t E_{(s_t, a_t) \\sim p_{\\theta}(s_t, a_t) }\\left[ r(s_t, a_t) \\right]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "#### Infinite horizon case: stationary distribution\n",
    "\\begin{eqnarray*}\n",
    "\\theta^{\\star} = arg \\max\\limits_{\\theta} \\sum\\limits_{t=1}^T E_{(s_t, a_t) \\sim p_{\\theta}(s_t, a_t) }\\left[ r(s_t, a_t) \\right]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Assumptions\n",
    "    - ergodicity\n",
    "    - $\\mu = T\\mu, (T-I)\\mu = 0$\n",
    "    - $\\mu$ is eigenvector of T with eigenvalue 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectations and stochastic systems\n",
    "- In reinforcement learning, we almost always care about increasing expected values (expectations).\n",
    "- Why reinforcement learning algorithms can optimize non-smooth reward functions?\n",
    "    - mainly because the expected values of non-smooth reward functions are smooth in $ \\theta $. (How?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Algorithms\n",
    "\n",
    "#### The anatomy of a reinforcement learning algorithm\n",
    "- Always contain three basic parts\n",
    "    1. Generate samples (i.e. run the policy) - trial stage.\\\n",
    "        (sample trajectories from the trajectory distribution, typically the one induced by our policy) \n",
    "    2. Fit a model/ estimate the return. (either model-based (dynamics) or implicit model (value-function))\n",
    "    3. Improve the policy. (Change the policy to make it better.)\n",
    "- Examples - Different incarnations of the second and third stage \n",
    "    1. (Policy gradient)\n",
    "        - Run policy $ \\rightarrow $ Evaluate policy $ \\rightarrow $ Improve policy\n",
    "        - generate trajectories $ \\rightarrow $ sum up the rewards $ \\rightarrow $ try to make good trajectories be more likely.\\\n",
    "        calculate and apply gradients.\n",
    "    2. RL by backprop (Model-based)\n",
    "    \n",
    "![image.png](..\\images\\rl_basics\\rl_anatomy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of algorithms\n",
    "1. Policy gradients: directly differentiate the above objective\n",
    "2. Value-based: estimate value function or Q-function of the optimal policy (no explicit policy)\n",
    "3. Actor-critic: estimate value function or Q-function of the current policy, use it to improve policy.\n",
    "4. Model-based RL: estimate the transition model, and then\n",
    "    - use it for planning (no explicit policy)\n",
    "        - Trajectory optimization/optimal control (primarily in continuous spaces) - essentially backpropagation to optimize\\\n",
    "            actions\n",
    "        - Discrete planning in discrete action spaces - e.g., Monte Carlo tree search\n",
    "    - use it to improve a policy (backpropogate gradients)\n",
    "        - Requires some tricks to make it work\n",
    "    - use the model to learn a value function\n",
    "        - Dynamic Programming\n",
    "        - Generate simulated experience for model free learner.\n",
    "    - something else\n",
    "\n",
    "#### Rollout, sample the policy i.e. run the policy one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradeoffs between algorithms\n",
    "(Why can't we just learn one RL algorithm that works for all tasks)\\\n",
    "- Different tradeoffs\n",
    "    - Sample efficiency: How many samples do we need from orange box\n",
    "    - Stability and ease of use: Multiple hyperparameters to select\n",
    "- Different assumptions\n",
    "    - Stochastic or deterministic?\n",
    "    - Continuous or discrete?\n",
    "    - Episodic or infinite horizon?\n",
    "- Different things are easy or hard in different settings\n",
    "    - Easier to represent the policy?\n",
    "    - Easier to represent the model?\n",
    "    \n",
    "#### Sample Efficiency\n",
    "- How many samples do we need to get a good policy?\n",
    "- Most important question: Is the algorithm off policy?\n",
    "    - Off policy: able to improve the policy without generating new samples from that policy\n",
    "    - On policy: each time the policy is changed, even a little bit, we need to generate new samples\n",
    "    \n",
    "![image.png](..\\images\\rl_basics\\sample_efficiency_rl_algorithms.png)\n",
    "##### Why would we use a less efficient algorithm?\n",
    "Wall clock time is not the same as efficiency!\n",
    "\n",
    "#### Stability and ease of use\n",
    "- Does it converge?\n",
    "- And if it converges, to what?\n",
    "- And does it converge every time?\n",
    "\n",
    "Why is any of this even a question???\n",
    "- Supervised learning: almost always gradient descent\n",
    "- Reinforcement learning: often not gradient descent\n",
    "\n",
    "#### Comparison: assumptions\n",
    "- common assumption #1: full observability\n",
    "    - generally assumed by value function fitting methods\n",
    "    - can be mitigated by adding recurrence\n",
    "- common assumption #2: episodic learning\n",
    "    - Often assumed by pure policy gradient methods\n",
    "    - assumed by some model-based RL methods\n",
    "- common assumption #3: continuity or smoothness\n",
    "    - Assumed by some continuous value function learning methods\n",
    "    - Often assumed by some model-based RL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of algorithms\n",
    "1. Value-based: estimate value function or Q-function of the optimal policy (no explicit policy)\n",
    "    - Q-learning, DQN\n",
    "    - Temporal difference learning\n",
    "    - Fitted value iteration\n",
    "2. Policy gradients: directly differentiate the above objective\n",
    "    - REINFORCE\n",
    "    - Natural policy gradient\n",
    "    - Trust region policy optimization (TRPO)\n",
    "    - Proximal Policy Optimization (PPO)\n",
    "3. Actor-critic: estimate value function or Q-function of the current policy, use it to improve policy.\n",
    "    - Asynchronous advantage actor-critic (A3C)\n",
    "    - Soft actor-critic (SAC)\n",
    "    - DDPg\n",
    "4. Model-based RL: estimate the transition model, and the then\n",
    "    - Dyna\n",
    "    - Guided policy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
